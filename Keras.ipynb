{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import App\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserMaker = App.UserMaker\n",
    "try:\n",
    "    with open('G:\\\\Programming\\\\major\\\\data\\\\users.dat', mode='rb') as file:\n",
    "        UM = pickle.load(file)\n",
    "except EOFError as E:\n",
    "    print(E)\n",
    "DM = App.DatasetMaker(UM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.User import User, UserData, UserType\n",
    "from models.Tweet import Tweet\n",
    "from random import shuffle\n",
    "id = DM.train_users[0].id\n",
    "\n",
    "all_train_users = DM.train_users\n",
    "all_test_users = DM.test_users\n",
    "\n",
    "shuffle(all_train_users)\n",
    "shuffle(all_test_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396864 1184025\n"
     ]
    }
   ],
   "source": [
    "x = list(filter(lambda x : x.user_type == UserType.BOT, all_train_users))\n",
    "y = list(filter(lambda x : x.user_type == UserType.HUMAN, all_train_users))\n",
    "y_sum = sum([user.num_tweets for user in y])\n",
    "x_sum = sum([user.num_tweets for user in x])\n",
    "print(x_sum, y_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_user_from_pickle(id):\n",
    "    with open('G:\\\\Programming\\\\major\\\\data\\\\processed_data\\\\' + id + '.dat', 'rb') as file:\n",
    "        U = pickle.load(file)\n",
    "    return U\n",
    "\n",
    "U = get_user_from_pickle(DM.train_users[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_epochs=20\n",
    "num_pool=2\n",
    "num_filters = 128\n",
    "conv_kernel_width = 3\n",
    "conv_kernel_height = 200\n",
    "lstm_output_size= 70\n",
    "dropout_rate = 0.1\n",
    "INPUT_UNITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Apps\\Conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "models = []\n",
    "for i in range(INPUT_UNITS):\n",
    "    tmodel = Sequential()\n",
    "    tmodel.add(Conv1D(num_filters, \n",
    "                      conv_kernel_width,\n",
    "                      padding='same',\n",
    "                      activation='relu',\n",
    "                      strides=1,\n",
    "                      input_shape=(20, 200)))\n",
    "    tmodel.add(MaxPooling1D(pool_size=20))\n",
    "    models.append(tmodel)\n",
    "\n",
    "fmodel = Sequential()\n",
    "fmodel.add(Merge(models, mode='concat'))\n",
    "fmodel.add(LSTM(64))\n",
    "fmodel.add(Dropout(0.1))\n",
    "fmodel.add(Dense(2))\n",
    "fmodel.add(Activation('sigmoid'))\n",
    "fmodel.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(num_filters,\n",
    "                 conv_kernel_width,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1,\n",
    "                 input_shape=(20, 200))) # (20x200) -> (20, 128) \n",
    "model.add(MaxPooling1D(pool_size=20)) # (20x128) -> (1, 128)  \n",
    "model.add(Reshape((128,1)))\n",
    "model.add(LSTM(batch_size))# \n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu')) # 64\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in fmodel.layers:\n",
    "    print(layer,end=',')\n",
    "    print(' input = ' + str(layer.input_shape) + ' output = ' + str(layer.output_shape))\n",
    "fmodel.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, dim_x = 20, dim_y = 200, batch_size=32 , shuffle=True):\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.batch_size = batch_size\n",
    "        self.user_batch_size = 100\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __get_exploration_order(self, user_list):\n",
    "        indexes = np.arange(len(user_list))\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            shuffle(indexes)\n",
    "        \n",
    "        return indexes\n",
    "    \n",
    "    def get_tweets_from_users(self, user_list):\n",
    "        vector_forms = []\n",
    "        labels = []\n",
    "        \n",
    "        for user in user_list:\n",
    "            U = get_user_from_pickle(user.id)\n",
    "            for tweet in U.tweets:\n",
    "                vector_forms.append(tweet.vector_form)\n",
    "                labels.append(U.user_type.value)\n",
    "        \n",
    "        imax = int(len(labels) / self.batch_size)\n",
    "        for i in range(imax):\n",
    "            temp_tweets = vector_forms[i*self.batch_size:(i+1) * self.batch_size]\n",
    "            temp_labels = labels[i*self.batch_size : (i+1) * self.batch_size]\n",
    "            \n",
    "            x, y = self.__data_generation(temp_tweets, temp_labels)\n",
    "            yield x,y\n",
    "    \n",
    "    \n",
    "    def generate(self, user_list):\n",
    "        while 1:\n",
    "            exploration_order = self.__get_exploration_order(user_list)\n",
    "            for i in range(len(user_list)):\n",
    "                temp_users = [user_list[k] for k in exploration_order[i*self.user_batch_size:\n",
    "                                                                      (i+1)*self.user_batch_size]]\n",
    "                for item in self.get_tweets_from_users(temp_users):\n",
    "                    yield item\n",
    "                \n",
    "    def __data_generation(self,  tweets, labels):\n",
    "        \n",
    "        X = np.empty((self.batch_size, self.dim_x, self.dim_y))\n",
    "        Y =  np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        for i, tweet in enumerate(tweets):\n",
    "            \n",
    "            X[i, :, :] = np.array(tweet)\n",
    "            Y[i] = labels[i]\n",
    "        \n",
    "        return X, np_utils.to_categorical(Y, 2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator2: \n",
    "    def __init__(self, dim_x = 20, dim_y = 200, batch_size = 1, shuffle=True ):\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.batch_size = batch_size\n",
    "        self.user_batch_size = 1\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __get_exploration_order(self, user_list):\n",
    "        indexes = np.arange(len(user_list))\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            shuffle(indexes)\n",
    "        return indexes\n",
    "    \n",
    "    def get_user_profile(self, user_list):\n",
    "        vector_forms = []\n",
    "        labels = []\n",
    "        \n",
    "        for index, user in enumerate(user_list):\n",
    "            U = get_user_from_pickle(user.id)\n",
    "            vector_forms.append([tweet.vector_form for tweet in U.tweets])\n",
    "            labels.append(U.user_type.value)\n",
    "            # print('list', index, ' ', len(vector_forms[index]), ' ', labels[index])\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            \n",
    "            #Vector forms is like [[user1_tweet1, user1_tweet2, ...], [user2_tweet1, user2_tweet2 ...]]\n",
    "            #Vector_forms is like [user1_history, user2_history, user3_history, ...]\n",
    "            #jmax gives us for the ith user, how many times the model can train on their tweets\n",
    "            \n",
    "            jmax = int(len(vector_forms[i]) / INPUT_UNITS) \n",
    "            \n",
    "            for j in range(jmax):\n",
    "                temp_tweets = [vector_forms[i][j * INPUT_UNITS : (j+1) * INPUT_UNITS]]\n",
    "                # print('l', len(temp_tweets))\n",
    "                temp_label = [labels[i]]\n",
    "                x, y = self.__data_generation(temp_tweets, temp_label)\n",
    "                yield x,y\n",
    "\n",
    "    \n",
    "    def generate(self, user_list):\n",
    "        while 1: \n",
    "            exploration_order = self.__get_exploration_order(user_list)\n",
    "            for i in range(len(user_list)):\n",
    "                temp_users = [user_list[k] for k in exploration_order[i*self.user_batch_size:\n",
    "                                                                      (i+1)*self.user_batch_size]]\n",
    "                for item in self.get_user_profile(temp_users):\n",
    "                    yield item\n",
    "                \n",
    "    def __data_generation(self,  tweets, labels):\n",
    "        \n",
    "        X = [np.empty((self.batch_size, self.dim_x, self.dim_y)) for i in range( INPUT_UNITS)]\n",
    "        Y =  np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "\n",
    "        for i in range(self.batch_size): \n",
    "            for k in range(INPUT_UNITS):\n",
    "                try:\n",
    "                    X[k][i, :, :] = np.array(tweets[i][k])\n",
    "                except IndexError as E:\n",
    "                    print('nexted', i, ' ', k, ' ', tweets[i])\n",
    "            Y[i] = labels[i]\n",
    "\n",
    "        return X, np_utils.to_categorical(Y, 2)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dim_x' : 20,\n",
    "    'dim_y' : 200,\n",
    "    'batch_size' : 32,\n",
    "    'shuffle' : True\n",
    "}\n",
    "batch_size = 32\n",
    "DM.train_tweets = sum([user.num_tweets for user in DM.train_users])\n",
    "DM.test_tweets = sum([user.num_tweets for user in DM.test_users])\n",
    "validation_generator = DataGenerator2().generate(DM.test_users)\n",
    "training_generator = DataGenerator2().generate(DM.train_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "7    'dim_x' : 20,\n",
    "    'dim_y' : 200,\n",
    "    'batch_size' : 32,\n",
    "    'shuffle' : True\n",
    "}\n",
    "batch_size = 32\n",
    "DM.train_tweets = sum([user.num_tweets for user in DM.train_users])\n",
    "DM.test_tweets = sum([user.num_tweets for user in DM.test_users])\n",
    "validation_generator = DataGenerator(**params).generate(DM.test_users)\n",
    "training_generator = DataGenerator(**params).generate(DM.train_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in training_generator:\n",
    "    x,y = item\n",
    "    print(len(x), y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "157424/157424 [==============================] - 10009s - loss: 11.7206 - acc: 0.2688 - val_loss: 12.1384 - val_acc: 0.2428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba01200e10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator = training_generator,\n",
    "                    steps_per_epoch = DM.train_tweets/10,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = DM.test_tweets/10,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('G:\\\\Programming\\\\major\\\\trained_model_user_nolstm')\n",
    "model.save_weights('G:\\\\Programming\\\\major\\\\ut_model_weights_nolstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, dim_x = 20, dim_y = 200, batch_size=32, shuffle=True):\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.batch_size = batch_size\n",
    "        self.user_batch_size = 100\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __get_exploration_order(self, user_list):\n",
    "        indexes = np.arange(len(user_list))\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            shuffle(indexes)\n",
    "        \n",
    "        return indexes\n",
    "    \n",
    "    def get_tweets_from_users(self, user_list):\n",
    "        vector_forms = []\n",
    "        labels = []\n",
    "        \n",
    "        for user in user_list:\n",
    "            U = get_user_from_pickle(user.id)\n",
    "            for tweet in U.tweets:\n",
    "                vector_forms.append(tweet.vector_form)\n",
    "                labels.append(U.user_type.value)\n",
    "        \n",
    "        imax = int(len(labels) / self.batch_size)\n",
    "        for i in range(imax):\n",
    "            temp_tweets = vector_forms[i*self.batch_size:(i+1) * self.batch_size]\n",
    "            temp_labels = labels[i*self.batch_size : (i+1) * self.batch_size]\n",
    "            \n",
    "            x, y = self.__data_generation(temp_tweets, temp_labels)\n",
    "            yield x,y\n",
    "    \n",
    "    def generate(self, user_list):\n",
    "        while 1:\n",
    "            exploration_order = self.__get_exploration_order(user_list)\n",
    "            for i in range(len(user_list)):\n",
    "                temp_users = [user_list[k] for k in exploration_order[i*self.user_batch_size:\n",
    "                                                                      (i+1)*self.user_batch_size]]\n",
    "                for item in self.get_tweets_from_users(temp_users):\n",
    "                    yield item\n",
    "                \n",
    "    def __data_generation(self,  tweets, labels):\n",
    "        \n",
    "        X = np.empty((self.batch_size, self.dim_x, self.dim_y))\n",
    "        Y =  np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        for i, tweet in enumerate(tweets):\n",
    "            \n",
    "            X[i, :, :] = np.array(tweet)\n",
    "            Y[i] = labels[i]\n",
    "        \n",
    "        return X, np_utils.to_categorical(Y, 2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Reshape\n",
    "n_model = []\n",
    "for i in range(batch_size):\n",
    "    n_model.append(Sequential())\n",
    "    n_model[i].add(Conv1D(num_filters,\n",
    "                     conv_kernel_width,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     strides=1,\n",
    "                     input_shape=(20,200,)))\n",
    "    n_model[i].add(MaxPooling1D(pool_size=20))\n",
    "\n",
    "dim1 = 1\n",
    "dim2 = 20 / num_pool\n",
    "dim3 = num_filters\n",
    "\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(Merge([m for m in n_model], mode='concat'))\n",
    "print(final_model.output_shape) # input_shape=[(20,200) for i in range(batch_size)]))#, input_shape=batch_size*num_filters)\n",
    "final_model.add(LSTM(lstm_output_size))\n",
    "final_model.add(Dropout(0.1))\n",
    "final_model.add(Dense(64, activation='relu'))\n",
    "final_model.add(Dense(2))\n",
    "final_model.add(Activation('softmax'))\n",
    "final_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit_generator(generator = training_generator,\n",
    "                    steps_per_epoch = DM.train_tweets//batch_size,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = DM.test_tweets//batch_size,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,11)], [89.9, 90.46, 91.47, 89.84, 89.87, 73.54, 89.82, 91.5, 91.03, 91.15])\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.title(\"Model Accuracy \")\n",
    "plt.axis([0,11,50,100])\n",
    "plt.xticks([i for i in range(1, 11)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"G:\\\\Programming\\\\major\\\\trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorize import vectorize\n",
    "from vectorize import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"This is a good thing\"\n",
    "vector_form = vectorize.vectorize_filter(\n",
    "                preprocess.lemmatize_filter(\n",
    "                    preprocess.tokenize_filter(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model.predict(np.reshape(np.array(vector_form), (1,20,200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
